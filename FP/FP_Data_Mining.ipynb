{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "# import emot\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('words')\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "tkn = TweetTokenizer()\n",
    "exclude = set(string.punctuation)\n",
    "StopWords = stopwords.words(\"english\")\n",
    "bag = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"C:/xampp/htdocs/Data Mining/FP/dataset/\"\n",
    "path_to_clean = \"C:/xampp/htdocs/Data Mining/FP/clean/\"\n",
    "# filename = \"SS-Twitter\"\n",
    "filename = \"STS-Gold\"\n",
    "# filename = \"STS-Test\"\n",
    "\n",
    "path_to_experiment = path_to_clean + filename +\"_EXPERIMENT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarity</th>\n",
       "      <th>NO URL</th>\n",
       "      <th>NO STOPWORD</th>\n",
       "      <th>NO NUMBER</th>\n",
       "      <th>NO REPEAT</th>\n",
       "      <th>NO ACRONYM</th>\n",
       "      <th>NO NEGATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "      <td>0</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>It looks as though Shaq is getting traded to C...</td>\n",
       "      <td>0</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>it loks as though shaq is getting traded to cl...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>looks though shaq getting traded cleveland pla...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
       "      <td>0</td>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "      <td>@clariane april th isn't coming soon enough</td>\n",
       "      <td>@clariane april ninth coming soon enough</td>\n",
       "      <td>@clarianne april th coming soon enough</td>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>drinking a McDonalds coffee and not understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking a mcdonalds coffee and not understand...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
       "      <td>0</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>so disapointed taylor swift doesnt have a twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>dissapointed taylor swift doesnt twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1467984364</td>\n",
       "      <td>Wishes I was on the Spring Fling Tour with Daw...</td>\n",
       "      <td>0</td>\n",
       "      <td>wishes spring fling tour dawn nece sigh g'knight</td>\n",
       "      <td>wishes i was on the spring fling tour with daw...</td>\n",
       "      <td>wishes spring fling tour dawn nece sigh g'knight</td>\n",
       "      <td>wishes spring fling tour dawn neecee sigh g'kn...</td>\n",
       "      <td>wishes spring fling tour dawn nece sigh g'knight</td>\n",
       "      <td>wishes spring fling tour dawn nece sigh g'knight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1964951623</td>\n",
       "      <td>got a sniffle, got the kids and hubby just lef...</td>\n",
       "      <td>0</td>\n",
       "      <td>got sniffle got kids hubby left work sydney we...</td>\n",
       "      <td>got a sniffle got the kids and hubby just left...</td>\n",
       "      <td>got sniffle got kids hubby left work sydney we...</td>\n",
       "      <td>got sniffle got kids hubby left work sydney we...</td>\n",
       "      <td>got sniffle got kids hubby left work sydney we...</td>\n",
       "      <td>got sniffle got kids hubby left work sydney we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1881133744</td>\n",
       "      <td>i've only been in sydney for 3 hrs but I miss ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i've sydney hours miss friends especially @ktjade</td>\n",
       "      <td>i've only been in sydney for hours but i miss ...</td>\n",
       "      <td>i've sydney 3 hours miss friends especially @k...</td>\n",
       "      <td>i've sydney hours miss friends especially @ktjade</td>\n",
       "      <td>i've sydney hrs miss friends especially @ktjade</td>\n",
       "      <td>i've sydney hours miss friends especially @ktjade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1753802024</td>\n",
       "      <td>xboxtweet not working again</td>\n",
       "      <td>0</td>\n",
       "      <td>xboxtwet working</td>\n",
       "      <td>xboxtwet not working again</td>\n",
       "      <td>xboxtwet working</td>\n",
       "      <td>xboxtweet working</td>\n",
       "      <td>xboxtwet working</td>\n",
       "      <td>xboxtwet working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1980497384</td>\n",
       "      <td>R.I.P to lebron/kobe puppet commercials...</td>\n",
       "      <td>0</td>\n",
       "      <td>p lebron kobe puppet comercials ...</td>\n",
       "      <td>are i p to lebron kobe puppet comercials ...</td>\n",
       "      <td>p lebron kobe puppet comercials ...</td>\n",
       "      <td>p lebron kobe puppet commercials ...</td>\n",
       "      <td>r p lebron kobe puppet comercials ...</td>\n",
       "      <td>p lebron kobe puppet comercials ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              tweet  polarity  \\\n",
       "0  1467933112  the angel is going to miss the athlete this we...         0   \n",
       "1  2323395086  It looks as though Shaq is getting traded to C...         0   \n",
       "2  1467968979     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH          0   \n",
       "3  1990283756  drinking a McDonalds coffee and not understand...         0   \n",
       "4  1988884918  So dissapointed Taylor Swift doesnt have a Twi...         0   \n",
       "5  1467984364  Wishes I was on the Spring Fling Tour with Daw...         0   \n",
       "6  1964951623  got a sniffle, got the kids and hubby just lef...         0   \n",
       "7  1881133744  i've only been in sydney for 3 hrs but I miss ...         0   \n",
       "8  1753802024                       xboxtweet not working again          0   \n",
       "9  1980497384        R.I.P to lebron/kobe puppet commercials...          0   \n",
       "\n",
       "                                              NO URL  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2              @clariane april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "5   wishes spring fling tour dawn nece sigh g'knight   \n",
       "6  got sniffle got kids hubby left work sydney we...   \n",
       "7  i've sydney hours miss friends especially @ktjade   \n",
       "8                                   xboxtwet working   \n",
       "9                p lebron kobe puppet comercials ...   \n",
       "\n",
       "                                         NO STOPWORD  \\\n",
       "0  the angel is going to miss the athlete this we...   \n",
       "1  it loks as though shaq is getting traded to cl...   \n",
       "2        @clariane april th isn't coming soon enough   \n",
       "3  drinking a mcdonalds coffee and not understand...   \n",
       "4  so disapointed taylor swift doesnt have a twitter   \n",
       "5  wishes i was on the spring fling tour with daw...   \n",
       "6  got a sniffle got the kids and hubby just left...   \n",
       "7  i've only been in sydney for hours but i miss ...   \n",
       "8                         xboxtwet not working again   \n",
       "9       are i p to lebron kobe puppet comercials ...   \n",
       "\n",
       "                                           NO NUMBER  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2           @clariane april ninth coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "5   wishes spring fling tour dawn nece sigh g'knight   \n",
       "6  got sniffle got kids hubby left work sydney we...   \n",
       "7  i've sydney 3 hours miss friends especially @k...   \n",
       "8                                   xboxtwet working   \n",
       "9                p lebron kobe puppet comercials ...   \n",
       "\n",
       "                                           NO REPEAT  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  looks though shaq getting traded cleveland pla...   \n",
       "2             @clarianne april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4           dissapointed taylor swift doesnt twitter   \n",
       "5  wishes spring fling tour dawn neecee sigh g'kn...   \n",
       "6  got sniffle got kids hubby left work sydney we...   \n",
       "7  i've sydney hours miss friends especially @ktjade   \n",
       "8                                  xboxtweet working   \n",
       "9               p lebron kobe puppet commercials ...   \n",
       "\n",
       "                                          NO ACRONYM  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2              @clariane april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "5   wishes spring fling tour dawn nece sigh g'knight   \n",
       "6  got sniffle got kids hubby left work sydney we...   \n",
       "7    i've sydney hrs miss friends especially @ktjade   \n",
       "8                                   xboxtwet working   \n",
       "9              r p lebron kobe puppet comercials ...   \n",
       "\n",
       "                                         NO NEGATION  \n",
       "0                   angel going miss athlete weekend  \n",
       "1  loks though shaq getting traded cleveland play...  \n",
       "2              @clariane april th coming soon enough  \n",
       "3  drinking mcdonalds coffee understanding someon...  \n",
       "4            disapointed taylor swift doesnt twitter  \n",
       "5   wishes spring fling tour dawn nece sigh g'knight  \n",
       "6  got sniffle got kids hubby left work sydney we...  \n",
       "7  i've sydney hours miss friends especially @ktjade  \n",
       "8                                   xboxtwet working  \n",
       "9                p lebron kobe puppet comercials ...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Loading dataset \n",
    "# Labels = [\"class\", \"id\", \"date\", \"query\", \"username\",\"tweet\"] #STS\n",
    "# Labels = [\"id\",\"polarity\",\"tweet\"] #Gold\n",
    "# Labels = [\"mean pos\",\"mean neg\",\"tweet\"] #SS\n",
    "\n",
    "# data = pd.read_csv(path_to_dataset + filename +\".csv\",names=Labels,sep=\";\",header=0) #Gold\n",
    "# data = pd.read_csv(path_to_dataset + filename +\".csv\",names=Labels) #STS\n",
    "# data = pd.read_csv(path_to_dataset + filename+\".csv\") #SS\n",
    "data = pd.read_csv(path_to_experiment) #EXPERIMENT\n",
    "\n",
    "\n",
    "# data = data[[\"id\", \"date\", \"query\", \"username\",\"tweet\",\"class\"]] #STS-Test\n",
    "# data = data[['id', 'tweet', 'polarity']] #STS-Gold\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing values:\n",
      "\tid: 0\n",
      "\ttweet: 0\n",
      "\tpolarity: 0\n",
      "\n",
      "Number of class values:\n",
      "0    1402\n",
      "4     632\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## CEK Bentuk data\n",
    "\n",
    "# Cek missing value\n",
    "\n",
    "print('\\nNumber of missing values:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col, data[col].isnull().sum()))\n",
    "    \n",
    "# cek jumlah perkelas\n",
    "print('\\nNumber of class values:')\n",
    "print(data.iloc[:,-1].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREPROCESSING\n",
    "\n",
    "## Tokenizing\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# #Repeated Word\n",
    "def normalize_repeated_char(token):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if old_word in bag:\n",
    "#         ps = PorterStemmer()\n",
    "#         ps.stem(old_word)\n",
    "#         if wordnet.synsets(old_word):\n",
    "#         if old_word in words.words():\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    correct = [replace(word) for word in token]\n",
    "    return correct\n",
    "\n",
    "# #Negation\n",
    "def normalized_negation_phrase(tokenized_sentence):\n",
    "\n",
    "    mydict = { \"did not\": [\"didn't\", \"didnt\"], \n",
    "              \"do not\": [\"don't\",\"dont\"], \n",
    "              \"can not\": [\"can't\",\"cant\"], \n",
    "              \"sould not\": [\"souldn't\",\"souldnt\"],\n",
    "              \"would not\": [\"wouldn't\",\"wouldnt\"], \n",
    "              \"could not\": [\"couldn't\",\"couldnt\"],\n",
    "              \"have not\": [\"haven't\",\"havent\"],\n",
    "              \"had not\": [\"hadn't\",\"hadnt\"],\n",
    "             }\n",
    "    \n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in mydict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue           \n",
    "    return tokenized_sentence\n",
    "\n",
    "# #Stopword\n",
    "# def stopword(tokenized_sentence):\n",
    "# #     StopWords = stopwords.words(\"english\")\n",
    "#     StopWords = stopwords.words(\"english\")\n",
    "# #     token = nltk.word_tokenize(contoh)\n",
    "#     for v in tokenized_sentence:\n",
    "#         if v not in StopWords:\n",
    "#             return ' '.join(tokenized_sentence)\n",
    "# #             print(v)\n",
    "\n",
    "    \n",
    "\n",
    "# #Akronim\n",
    "def normalize_slang_words(tokenized_sentence):\n",
    "    slang_word_dict = json.loads(open(\"slang_word_dict.txt\", 'r').read())\n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in slang_word_dict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue           \n",
    "    return tokenized_sentence\n",
    "\n",
    "# #Emoticon\n",
    "def emoticon(tokenized_sentence):\n",
    "    emoticon_dict = json.loads(open(\"emoticon_dict.txt\", 'r').read())\n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in emoticon_dict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue           \n",
    "    return tokenized_sentence\n",
    "\n",
    "# Join\n",
    "# def join_token(tokenized_sentence):\n",
    "#     for index in range(len(tokenized_sentence)):\n",
    "#         tokenized_sentence[index] = '|'.join(str(i) for i in tokenized_sentence[index])\n",
    "#     return ' '.join(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(comment):\n",
    "    \n",
    "    comment = comment.lower() #lower\n",
    "    comment = html.unescape(comment) #html tag\n",
    "    comment = re.sub(\"@[A-Za-z0-9]+\", \"\", comment) #no Mention\n",
    "    comment = re.sub(\"#[A-Za-z0-9]+\", \"\", comment) #no Hashtag\n",
    "    comment = re.sub(\"[0-9]\", \"\", comment) #no Number\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment) #no HTML    \n",
    "    comment = tkn.tokenize(comment)#token\n",
    "    comment = emoticon(comment) #Emoticon\n",
    "    comment = normalize_repeated_char(comment) #Repeated\n",
    "    comment = normalized_negation_phrase(comment) #Negation\n",
    "    comment = normalize_slang_words(comment) #slang\n",
    "    comment = [word for word in comment if word not in StopWords] #Stopword\n",
    "    comment = ' '.join(ch for ch in comment if ch not in exclude) #Puntuation\n",
    "    \n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarity</th>\n",
       "      <th>NO URL</th>\n",
       "      <th>NO STOPWORD</th>\n",
       "      <th>NO NUMBER</th>\n",
       "      <th>NO REPEAT</th>\n",
       "      <th>NO ACRONYM</th>\n",
       "      <th>NO NEGATION</th>\n",
       "      <th>OUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "      <td>0</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>It looks as though Shaq is getting traded to C...</td>\n",
       "      <td>0</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>it loks as though shaq is getting traded to cl...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>looks though shaq getting traded cleveland pla...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
       "      <td>0</td>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "      <td>@clariane april th isn't coming soon enough</td>\n",
       "      <td>@clariane april ninth coming soon enough</td>\n",
       "      <td>@clarianne april th coming soon enough</td>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "      <td>april th coming soon enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>drinking a McDonalds coffee and not understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking a mcdonalds coffee and not understand...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
       "      <td>0</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>so disapointed taylor swift doesnt have a twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>dissapointed taylor swift doesnt twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              tweet  polarity  \\\n",
       "0  1467933112  the angel is going to miss the athlete this we...         0   \n",
       "1  2323395086  It looks as though Shaq is getting traded to C...         0   \n",
       "2  1467968979     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH          0   \n",
       "3  1990283756  drinking a McDonalds coffee and not understand...         0   \n",
       "4  1988884918  So dissapointed Taylor Swift doesnt have a Twi...         0   \n",
       "\n",
       "                                              NO URL  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2              @clariane april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "\n",
       "                                         NO STOPWORD  \\\n",
       "0  the angel is going to miss the athlete this we...   \n",
       "1  it loks as though shaq is getting traded to cl...   \n",
       "2        @clariane april th isn't coming soon enough   \n",
       "3  drinking a mcdonalds coffee and not understand...   \n",
       "4  so disapointed taylor swift doesnt have a twitter   \n",
       "\n",
       "                                           NO NUMBER  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2           @clariane april ninth coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "\n",
       "                                           NO REPEAT  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  looks though shaq getting traded cleveland pla...   \n",
       "2             @clarianne april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4           dissapointed taylor swift doesnt twitter   \n",
       "\n",
       "                                          NO ACRONYM  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2              @clariane april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "\n",
       "                                         NO NEGATION  \\\n",
       "0                   angel going miss athlete weekend   \n",
       "1  loks though shaq getting traded cleveland play...   \n",
       "2              @clariane april th coming soon enough   \n",
       "3  drinking mcdonalds coffee understanding someon...   \n",
       "4            disapointed taylor swift doesnt twitter   \n",
       "\n",
       "                                                 OUR  \n",
       "0                   angel going miss athlete weekend  \n",
       "1  loks though shaq getting traded cleveland play...  \n",
       "2                        april th coming soon enough  \n",
       "3  drinking mcdonalds coffee understanding someon...  \n",
       "4            disapointed taylor swift doesnt twitter  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['OUR'] = data['tweet'].apply(preprocessing)\n",
    "data.head()\n",
    "# data['Clean'].loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Clean Data\n",
    "\n",
    "# data.to_csv(path_to_clean+filename+\"_BASELINE.csv\",index=False) \n",
    "# #with url, stopword, number, repeated, acronym, negation\n",
    "\n",
    "data.to_csv(path_to_clean+filename+\"_EXPERIMENT.csv\",index=False) \n",
    "# #combination of url, stopword, number, repeated, acronym, negation, our preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Make Dict\n",
    "\n",
    "# # #Join\n",
    "# file1 = pd.read_csv(path_to_clean + \"STS-Test.csv\")\n",
    "# file2 = pd.read_csv(path_to_clean + \"SS-Twitter.csv\")\n",
    "# file3 = pd.read_csv(path_to_clean + \"STS-Gold.csv\")\n",
    "\n",
    "# s1 = file1['Clean']\n",
    "# s2 = file2['Clean']\n",
    "# s3 = file3['Clean']\n",
    "\n",
    "# tweet = list()\n",
    "# tweet = s1.append(s2)\n",
    "# tweet = tweet.append(s3)\n",
    "# # tweet.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Save\n",
    "# tweet.to_csv(path_to_clean+\"All_Tweet.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tweet = pd.read_csv(path_to_clean + \"All_Tweet.csv\",header=None)\n",
    "# all_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Emoticon Dict\n",
    "\n",
    "# for index, row in all_tweet.iterrows():\n",
    "#     ContentnoMention = re.sub(\"[A-Za-z]\",\"\",str(row))\n",
    "\n",
    "#     print(ContentnoMention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Check kata baku\n",
    "# def is_english_word(comment):\n",
    "#     # creation of this dictionary would be done outside of \n",
    "#     #     the function because you only need to do it once.\n",
    "#     comment = comment.lower() #lower\n",
    "#     comment = html.unescape(comment) #html tag\n",
    "#     comment = re.sub(\"@[A-Za-z0-9]+\", \"\", comment) #no Mention\n",
    "#     comment = re.sub(\"#[A-Za-z0-9]+\", \"\", comment) #no Hashtag\n",
    "#     comment = re.sub(\"[0-9]\", \"\", comment) #no Number\n",
    "#     comment = re.sub(r\"http\\S+\", \"\", comment) #no HTML    \n",
    "#     comment = tkn.tokenize(comment)#token\n",
    "#     dictionary = dict.fromkeys(words.words(), None)\n",
    "#     for i,v in enumerate(comment):\n",
    "#         try:\n",
    "#             x = dictionary[v]\n",
    "#             continue\n",
    "#         except KeyError:\n",
    "#             print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Tweet'].apply(is_english_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrapping\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests, json\n",
    "# resp = requests.get('http://www.netlingo.com/acronyms.php')\n",
    "# soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "# slangdict= {}\n",
    "# key=\"\"\n",
    "# value=\"\"\n",
    "# for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "#     for li in div.findAll('li'):\n",
    "#         for a in li.findAll('a'):\n",
    "#             key =a.text\n",
    "#         value = li.text.split(key)[1]\n",
    "#         slangdict[key]=value\n",
    "# with open('C:/Users/USER/Downloads/tegar nitip/SMT 8/DatMin/FP/myslang.json', 'w') as find:\n",
    "#     json.dump(slangdict,find,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to csv\n",
    "\n",
    "# data = pd.read_csv(path_to_dataset + filename, delimiter=\"\\t\", encoding='latin1')\n",
    "# data.to_csv(path_to_dataset+\"SS-Twitter.csv\",index=False)\n",
    "# # f = open(path_to_dataset + filename, \"r\")\n",
    "# # content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
