{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\5115100178\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\5115100178\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\5115100178\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tkn = TweetTokenizer()\n",
    "StopWords = stopwords.words(\"english\")\n",
    "bag = words.words()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path_to_dataset = \"C:/xampp/htdocs/Data Mining/FP/dataset/\"\n",
    "path_to_clean = \"F:/178/Data-Mining/FP/clean/\"\n",
    "path_to_fitur = \"F:/178/Data-Mining/FP/fitur/\"\n",
    "filename = \"SS-Twitter\"\n",
    "# filename = \"STS-Gold\"\n",
    "# filename = \"STS-Test\"\n",
    "\n",
    "path_to_experiment = path_to_clean + filename +\"_BASELINE.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean pos</th>\n",
       "      <th>mean neg</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>?RT @justinbiebcr: The bigger the better....if...</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Listening to the \"New Age\" station on @Slacker...</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I favorited a YouTube video -- Drake and Josh ...</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>i didnt mean knee high I ment in lengt it goes...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I wana see the vid Kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean pos  mean neg                                              Tweet  \\\n",
       "0         3         2  ?RT @justinbiebcr: The bigger the better....if...   \n",
       "1         3         1  Listening to the \"New Age\" station on @Slacker...   \n",
       "2         1         1  I favorited a YouTube video -- Drake and Josh ...   \n",
       "3         4         2  i didnt mean knee high I ment in lengt it goes...   \n",
       "4         2         1                            I wana see the vid Kyan   \n",
       "\n",
       "                                               Clean  \n",
       "0    rt @justinbiebcr bigger better ... know mean ;)  \n",
       "1             listening new age station @slackeradio  \n",
       "2      favorited youtube video drake josh storm rock  \n",
       "3  did not mean knee high ment lengt goes knes cu...  \n",
       "4                               wanna see video kyan  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = pd.read_csv(path_to_experiment) #STS\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(tkn.tokenize(text), n)\n",
    "    \n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def get_pos(x):\n",
    "    for pos in x:\n",
    "        tampung = pos[1]\n",
    "        \n",
    "        if tampung.startswith('J'): #adj\n",
    "            return 'a'\n",
    "        elif tampung.startswith('V'): #verb\n",
    "            return 'v'\n",
    "        elif tampung.startswith('N'): #noun\n",
    "            return 'n'\n",
    "        elif tampung.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %reset_selective <regular_expression>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-1a30bda249fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfitur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mn_gram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mpos_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-0a11cf06c7a5>\u001b[0m in \u001b[0;36mget_ngrams\u001b[1;34m(text, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mn_grams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtkn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrams\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py36\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[1;31m# Fix HTML character entities:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_replace_html_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;31m# Remove username handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip_handles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py36\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36m_replace_html_entities\u001b[1;34m(text, keep, remove_illegal, encoding)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mremove_illegal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mENT_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_entity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_str_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#get value for tweet\n",
    "# gram_name = [\"UNIGRAM\",\"BIGRAM\",\"TRIGRAM\"]?\n",
    "\n",
    "fitur = list()\n",
    "for index, word in enumerate(base['Clean']):\n",
    "    n_gram = get_ngrams(word, 1)\n",
    "    pos_tags = nltk.pos_tag(n_gram)\n",
    "    var = get_pos(pos_tags)\n",
    "\n",
    "    lemma = [lemmatizer.lemmatize(gram, pos = var) for gram in n_gram]\n",
    "#     print(lemma)\n",
    "\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    for x in lemma:\n",
    "        synsets = wn.synsets(x, pos=var)\n",
    "        for synset in synsets :\n",
    "            synset = synsets[0]\n",
    "            senti = swn.senti_synset(synset.name())\n",
    "\n",
    "            sentiment += senti.pos_score() - senti.neg_score()\n",
    "            tokens_count += 1\n",
    "            \n",
    "    # judgment call ? Default to positive or negative\n",
    "    if sentiment > 0:\n",
    "        classes = 4\n",
    "        fitur.append(classes)\n",
    " \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    else:\n",
    "        classes = 0\n",
    "        fitur.append(classes)\n",
    "        \n",
    "    # negative sentiment\n",
    "#     else:\n",
    "#         classes = 0\n",
    "#         fitur.append(classes)\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data={\"tweet\": base['Clean'].to_list(), \"sentiment class\": fitur})\n",
    "df.to_csv(path_to_fitur +filename+\"_UNIGRAM.csv\", sep=',',index=False)\n",
    "fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loks though shaq getting traded cleveland play...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@clariane april th coming soon enough</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disapointed taylor swift doesnt twitter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment class\n",
       "0                   angel going miss athlete weekend                0\n",
       "1  loks though shaq getting traded cleveland play...                0\n",
       "2              @clariane april th coming soon enough                0\n",
       "3  drinking mcdonalds coffee understanding someon...                0\n",
       "4            disapointed taylor swift doesnt twitter                0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni = pd.read_csv(path_to_fitur+ filename +\"_UNIGRAM.csv\")\n",
    "uni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi = pd.read_csv(path_to_fitur +\"_BIGRAM.csv\")\n",
    "# bi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri = pd.read_csv(path_to_fitur +\"_TRIGRAM.csv\")\n",
    "# tri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array(fitur)\n",
    "# y = base['class'].values\n",
    "# SEED = 10\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, \n",
    "#                                                     y, \n",
    "#                                                     test_size=.2, \n",
    "#                                                     random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive, {2:.2f}% neutral\"\n",
    "#     .format(len(x_train),\n",
    "#     (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "#     (len(x_train[y_train == 4]) / (len(x_train)*1.))*100,\n",
    "#     (len(x_train[y_train == 2]) / (len(x_train)*1.))*100))\n",
    "\n",
    "# print (\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive, {2:.2f}% neutral\"\n",
    "# .format(len(x_test),\n",
    "# (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "# (len(x_test[y_test == 4]) / (len(x_test)*1.))*100,\n",
    "# (len(x_test[y_test == 2]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/178/Data-Mining/FP/fitur/STS-Test_UNIGRAM.csv\n"
     ]
    }
   ],
   "source": [
    "print(path_to_fitur +filename+\"_UNIGRAM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
