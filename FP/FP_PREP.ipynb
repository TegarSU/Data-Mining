{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\5115100178\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\5115100178\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "# import emot\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('words')\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "tkn = TweetTokenizer()\n",
    "exclude = set(string.punctuation)\n",
    "StopWords = stopwords.words(\"english\")\n",
    "bag = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_dataset = \"C:/xampp/htdocs/Data-Mining/FP/dataset/\"\n",
    "path_to_clean = \"C:/xampp/htdocs/Data-Mining/FP/clean/\"\n",
    "filename = \"SS-Twitter\"\n",
    "# filename = \"STS-Gold\" #\n",
    "# filename = \"STS-Test\"\n",
    "\n",
    "path_to_experiment = path_to_clean + filename +\"_EXPERIMENT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean pos</th>\n",
       "      <th>mean neg</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clean</th>\n",
       "      <th>class</th>\n",
       "      <th>NO URL</th>\n",
       "      <th>NO STOPWORD</th>\n",
       "      <th>NO NUMBER</th>\n",
       "      <th>NO REPEAT</th>\n",
       "      <th>NO ACRONYM</th>\n",
       "      <th>NO NEGATION</th>\n",
       "      <th>OUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>?RT @justinbiebcr: The bigger the better....if...</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>4</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr the bigger the better ... if ...</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt bigger better ... know mean happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Listening to the \"New Age\" station on @Slacker...</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>4</td>\n",
       "      <td>listening new age station @slackeradio htp://s...</td>\n",
       "      <td>listening to the new age station on @slackeradio</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>listening new age station @slackerradio</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>listening new age station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I favorited a YouTube video -- Drake and Josh ...</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>2</td>\n",
       "      <td>favorited youtube video drake josh storm rock ...</td>\n",
       "      <td>i favorited a youtube video drake and josh the...</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>i didnt mean knee high I ment in lengt it goes...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>4</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>i did not mean knee high i ment in lengt it go...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>did not mean knee high ment lengt goes knees c...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I wana see the vid Kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>4</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>i wanna see the video kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>wana see vid kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean pos  mean neg                                              Tweet  \\\n",
       "0         3         2  ?RT @justinbiebcr: The bigger the better....if...   \n",
       "1         3         1  Listening to the \"New Age\" station on @Slacker...   \n",
       "2         1         1  I favorited a YouTube video -- Drake and Josh ...   \n",
       "3         4         2  i didnt mean knee high I ment in lengt it goes...   \n",
       "4         2         1                            I wana see the vid Kyan   \n",
       "\n",
       "                                               Clean  class  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)      4   \n",
       "1             listening new age station @slackeradio      4   \n",
       "2      favorited youtube video drake josh storm rock      2   \n",
       "3  did not mean knee high ment lengt goes knes cu...      4   \n",
       "4                               wanna see video kyan      4   \n",
       "\n",
       "                                              NO URL  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1  listening new age station @slackeradio htp://s...   \n",
       "2  favorited youtube video drake josh storm rock ...   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                         NO STOPWORD  \\\n",
       "0  rt @justinbiebcr the bigger the better ... if ...   \n",
       "1   listening to the new age station on @slackeradio   \n",
       "2  i favorited a youtube video drake and josh the...   \n",
       "3  i did not mean knee high i ment in lengt it go...   \n",
       "4                         i wanna see the video kyan   \n",
       "\n",
       "                                           NO NUMBER  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1             listening new age station @slackeradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                           NO REPEAT  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1            listening new age station @slackerradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knees c...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                          NO ACRONYM  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1             listening new age station @slackeradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                                  wana see vid kyan   \n",
       "\n",
       "                                         NO NEGATION  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1             listening new age station @slackeradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                                 OUR  \n",
       "0               rt bigger better ... know mean happy  \n",
       "1                          listening new age station  \n",
       "2      favorited youtube video drake josh storm rock  \n",
       "3  did not mean knee high ment lengt goes knes cu...  \n",
       "4                               wanna see video kyan  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # #Loading dataset \n",
    "# # Labels = [\"class\", \"id\", \"date\", \"query\", \"username\",\"tweet\"] #STS\n",
    "# # Labels = [\"id\",\"polarity\",\"tweet\"] #Gold\n",
    "Labels = [\"mean pos\",\"mean neg\",\"tweet\"] #SS\n",
    "\n",
    "# # data = pd.read_csv(path_to_dataset + filename +\".csv\",names=Labels,sep=\";\",header=0) #Gold\n",
    "# # data = pd.read_csv(path_to_dataset + filename +\".csv\",names=Labels) #STS\n",
    "# data = pd.read_csv(path_to_dataset + filename+\".csv\") #SS\n",
    "data = pd.read_csv(path_to_experiment) #EXPERIMENT\n",
    "\n",
    "\n",
    "# # data = data[[\"id\", \"date\", \"query\", \"username\",\"tweet\",\"class\"]] #STS-Test\n",
    "# # data = data[['id', 'tweet', 'polarity']] #STS-Gold\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## CEK Bentuk data\n",
    "\n",
    "# # Cek missing value\n",
    "\n",
    "# print('\\nNumber of missing values:')\n",
    "# for col in data.columns:\n",
    "#     print('\\t%s: %d' % (col, data[col].isnull().sum()))\n",
    "    \n",
    "# # cek jumlah perkelas\n",
    "# print('\\nNumber of class values:')\n",
    "# print(data.iloc[:,-1].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### PREPROCESSING\n",
    "\n",
    "## Tokenizing\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# #Repeated Word\n",
    "def normalize_repeated_char(token):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if old_word in bag:\n",
    "#         ps = PorterStemmer()\n",
    "#         ps.stem(old_word)\n",
    "#         if wordnet.synsets(old_word):\n",
    "#         if old_word in words.words():\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    correct = [replace(word) for word in token]\n",
    "    return correct\n",
    "\n",
    "# #Negation\n",
    "def normalized_negation_phrase(tokenized_sentence):\n",
    "\n",
    "    mydict = { \"did not\": [\"didn't\", \"didnt\"], \n",
    "              \"do not\": [\"don't\",\"dont\"], \n",
    "              \"can not\": [\"can't\",\"cant\"], \n",
    "              \"sould not\": [\"souldn't\",\"souldnt\"],\n",
    "              \"would not\": [\"wouldn't\",\"wouldnt\"], \n",
    "              \"could not\": [\"couldn't\",\"couldnt\"],\n",
    "              \"have not\": [\"haven't\",\"havent\"],\n",
    "              \"had not\": [\"hadn't\",\"hadnt\"],\n",
    "             }\n",
    "    \n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in mydict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue           \n",
    "    return tokenized_sentence\n",
    "\n",
    "# #Stopword\n",
    "# def stopword(tokenized_sentence):\n",
    "# #     StopWords = stopwords.words(\"english\")\n",
    "#     StopWords = stopwords.words(\"english\")\n",
    "# #     token = nltk.word_tokenize(contoh)\n",
    "#     for v in tokenized_sentence:\n",
    "#         if v not in StopWords:\n",
    "#             return ' '.join(tokenized_sentence)\n",
    "# #             print(v)\n",
    "\n",
    "    \n",
    "\n",
    "# #Akronim\n",
    "def normalize_slang_words(tokenized_sentence):\n",
    "    slang_word_dict = json.loads(open(\"slang_word_dict.txt\", 'r').read())\n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in slang_word_dict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue           \n",
    "    return tokenized_sentence\n",
    "\n",
    "# #Emoticon\n",
    "def emoticon(tokenized_sentence):\n",
    "    emoticon_dict = json.loads(open(\"emoticon_dict.txt\", 'r').read())\n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in emoticon_dict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue           \n",
    "    return tokenized_sentence\n",
    "\n",
    "# Join\n",
    "# def join_token(tokenized_sentence):\n",
    "#     for index in range(len(tokenized_sentence)):\n",
    "#         tokenized_sentence[index] = '|'.join(str(i) for i in tokenized_sentence[index])\n",
    "#     return ' '.join(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(comment):\n",
    "    \n",
    "    comment = comment.lower() #lower\n",
    "    comment = html.unescape(comment) #html tag\n",
    "    comment = re.sub(\"@[A-Za-z0-9]+\", \"\", comment) #no Mention\n",
    "    comment = re.sub(\"#[A-Za-z0-9]+\", \"\", comment) #no Hashtag\n",
    "    comment = re.sub(\"[0-9]\", \"\", comment) #no Number\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment) #no HTML    \n",
    "    comment = tkn.tokenize(comment)#token\n",
    "    comment = emoticon(comment) #Emoticon\n",
    "    comment = normalize_repeated_char(comment) #Repeated\n",
    "    comment = normalized_negation_phrase(comment) #Negation\n",
    "    comment = normalize_slang_words(comment) #slang\n",
    "    comment = [word for word in comment if word not in StopWords] #Stopword\n",
    "    comment = ' '.join(ch for ch in comment if ch not in exclude) #Puntuation\n",
    "    \n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean pos</th>\n",
       "      <th>mean neg</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clean</th>\n",
       "      <th>class</th>\n",
       "      <th>NO URL</th>\n",
       "      <th>NO STOPWORD</th>\n",
       "      <th>NO NUMBER</th>\n",
       "      <th>NO REPEAT</th>\n",
       "      <th>NO ACRONYM</th>\n",
       "      <th>NO NEGATION</th>\n",
       "      <th>OUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>?RT @justinbiebcr: The bigger the better....if...</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>4</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr the bigger the better ... if ...</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt @justinbiebcr bigger better ... know mean ;)</td>\n",
       "      <td>rt bigger better ... know mean happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Listening to the \"New Age\" station on @Slacker...</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>4</td>\n",
       "      <td>listening new age station @slackeradio htp://s...</td>\n",
       "      <td>listening to the new age station on @slackeradio</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>listening new age station @slackerradio</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>listening new age station @slackeradio</td>\n",
       "      <td>listening new age station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I favorited a YouTube video -- Drake and Josh ...</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>2</td>\n",
       "      <td>favorited youtube video drake josh storm rock ...</td>\n",
       "      <td>i favorited a youtube video drake and josh the...</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "      <td>favorited youtube video drake josh storm rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>i didnt mean knee high I ment in lengt it goes...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>4</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>i did not mean knee high i ment in lengt it go...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>did not mean knee high ment lengt goes knees c...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "      <td>did not mean knee high ment lengt goes knes cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I wana see the vid Kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>4</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>i wanna see the video kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>wana see vid kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "      <td>wanna see video kyan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean pos  mean neg                                              Tweet  \\\n",
       "0         3         2  ?RT @justinbiebcr: The bigger the better....if...   \n",
       "1         3         1  Listening to the \"New Age\" station on @Slacker...   \n",
       "2         1         1  I favorited a YouTube video -- Drake and Josh ...   \n",
       "3         4         2  i didnt mean knee high I ment in lengt it goes...   \n",
       "4         2         1                            I wana see the vid Kyan   \n",
       "\n",
       "                                               Clean  class  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)      4   \n",
       "1             listening new age station @slackeradio      4   \n",
       "2      favorited youtube video drake josh storm rock      2   \n",
       "3  did not mean knee high ment lengt goes knes cu...      4   \n",
       "4                               wanna see video kyan      4   \n",
       "\n",
       "                                              NO URL  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1  listening new age station @slackeradio htp://s...   \n",
       "2  favorited youtube video drake josh storm rock ...   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                         NO STOPWORD  \\\n",
       "0  rt @justinbiebcr the bigger the better ... if ...   \n",
       "1   listening to the new age station on @slackeradio   \n",
       "2  i favorited a youtube video drake and josh the...   \n",
       "3  i did not mean knee high i ment in lengt it go...   \n",
       "4                         i wanna see the video kyan   \n",
       "\n",
       "                                           NO NUMBER  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1             listening new age station @slackeradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                           NO REPEAT  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1            listening new age station @slackerradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knees c...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                          NO ACRONYM  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1             listening new age station @slackeradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                                  wana see vid kyan   \n",
       "\n",
       "                                         NO NEGATION  \\\n",
       "0    rt @justinbiebcr bigger better ... know mean ;)   \n",
       "1             listening new age station @slackeradio   \n",
       "2      favorited youtube video drake josh storm rock   \n",
       "3  did not mean knee high ment lengt goes knes cu...   \n",
       "4                               wanna see video kyan   \n",
       "\n",
       "                                                 OUR  \n",
       "0               rt bigger better ... know mean happy  \n",
       "1                          listening new age station  \n",
       "2      favorited youtube video drake josh storm rock  \n",
       "3  did not mean knee high ment lengt goes knes cu...  \n",
       "4                               wanna see video kyan  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['OUR'] = data['Tweet'].apply(preprocessing)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Save Clean Data\n",
    "\n",
    "# data.to_csv(path_to_clean+filename+\"_BASELINE.csv\",index=False) \n",
    "# # #with url, stopword, number, repeated, acronym, negation\n",
    "\n",
    "data.to_csv(path_to_clean+filename+\"_EXPERIMENT.csv\",index=False) \n",
    "# # #combination of url, stopword, number, repeated, acronym, negation, our preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # #Make Dict\n",
    "\n",
    "# # #Join\n",
    "# file1 = pd.read_csv(path_to_clean + \"STS-Test.csv\")\n",
    "# file2 = pd.read_csv(path_to_clean + \"SS-Twitter.csv\")\n",
    "# file3 = pd.read_csv(path_to_clean + \"STS-Gold.csv\")\n",
    "\n",
    "# s1 = file1['Clean']\n",
    "# s2 = file2['Clean']\n",
    "# s3 = file3['Clean']\n",
    "\n",
    "# tweet = list()\n",
    "# tweet = s1.append(s2)\n",
    "# tweet = tweet.append(s3)\n",
    "# # tweet.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # #Save\n",
    "# tweet.to_csv(path_to_clean+\"All_Tweet.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_tweet = pd.read_csv(path_to_clean + \"All_Tweet.csv\",header=None)\n",
    "# all_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # Emoticon Dict\n",
    "\n",
    "# for index, row in all_tweet.iterrows():\n",
    "#     ContentnoMention = re.sub(\"[A-Za-z]\",\"\",str(row))\n",
    "\n",
    "#     print(ContentnoMention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # #Check kata baku\n",
    "# def is_english_word(comment):\n",
    "#     # creation of this dictionary would be done outside of \n",
    "#     #     the function because you only need to do it once.\n",
    "#     comment = comment.lower() #lower\n",
    "#     comment = html.unescape(comment) #html tag\n",
    "#     comment = re.sub(\"@[A-Za-z0-9]+\", \"\", comment) #no Mention\n",
    "#     comment = re.sub(\"#[A-Za-z0-9]+\", \"\", comment) #no Hashtag\n",
    "#     comment = re.sub(\"[0-9]\", \"\", comment) #no Number\n",
    "#     comment = re.sub(r\"http\\S+\", \"\", comment) #no HTML    \n",
    "#     comment = tkn.tokenize(comment)#token\n",
    "#     dictionary = dict.fromkeys(words.words(), None)\n",
    "#     for i,v in enumerate(comment):\n",
    "#         try:\n",
    "#             x = dictionary[v]\n",
    "#             continue\n",
    "#         except KeyError:\n",
    "#             print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data['Tweet'].apply(is_english_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Scrapping\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests, json\n",
    "# resp = requests.get('http://www.netlingo.com/acronyms.php')\n",
    "# soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "# slangdict= {}\n",
    "# key=\"\"\n",
    "# value=\"\"\n",
    "# for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "#     for li in div.findAll('li'):\n",
    "#         for a in li.findAll('a'):\n",
    "#             key =a.text\n",
    "#         value = li.text.split(key)[1]\n",
    "#         slangdict[key]=value\n",
    "# with open('C:/Users/USER/Downloads/tegar nitip/SMT 8/DatMin/FP/myslang.json', 'w') as find:\n",
    "#     json.dump(slangdict,find,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to csv\n",
    "\n",
    "# data = pd.read_csv(path_to_dataset + filename, delimiter=\"\\t\", encoding='latin1')\n",
    "# data.to_csv(path_to_dataset+\"SS-Twitter.csv\",index=False)\n",
    "# # f = open(path_to_dataset + filename, \"r\")\n",
    "# # content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path_to_clean = \"C:/xampp/htdocs/Data-Mining/FP/dataset/\"\n",
    "\n",
    "# Class = list()\n",
    "# pos = base['mean pos'].to_list()\n",
    "# neg = base['mean neg'].to_list()\n",
    "\n",
    "# for i, val in zip(pos,neg):\n",
    "#     senti = i - val\n",
    "#     if senti < 0:\n",
    "#         Class.append(0)\n",
    "#     elif senti > 0:\n",
    "#         Class.append(4)\n",
    "#     else:\n",
    "#         Class.append(2)\n",
    "\n",
    "# base['class'] = Class\n",
    "# base.head()\n",
    "\n",
    "# base.to_csv(path_to_clean+filename+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holiday', 'time', 'to']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
